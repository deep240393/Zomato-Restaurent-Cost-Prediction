# Zomato-Restaurent-Cost-Prediction

**Introduction**
---
This dataset provides information about various restaurant of Banglore which are registered in the Zomato. As
Banglore is considered the IT hub of India, there is a huge amount of competition among different Restaurants.
If someone is going to establish a new restaurant in the city, this dataset is helpful to get an insight into various
restaurants rating, cuisine, location etc

# imported different libraries for performing Assignment
import pandas as pd
import numpy as np
import seaborn as sns; sns.set()
import matplotlib.pyplot as plot
import matplotlib.pyplot as figure
import difflib
import warnings
import xgboost as xgb
from ReliefF import ReliefF
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, learning_curve, validation_curve
from sklearn.model_selection import cross_val_score
warnings.simplefilter('ignore')
from sklearn import cross_validation
from sklearn.metrics import accuracy_score,mean_squared_error,mean_absolute_error,r2_score
from sklearn.model_selection import GridSearchCV,train_test_split

**Supervised Learning Method**
---
We are solving the problem of Supervised Learning.
Supervised learning is the Data mining task of inferring a function from labelled training data. If a specific target
is provided, then the problem can be phrased as a supervised one. Target value should exist in the principles as
well as data.In this problem, our target variable is defined, which is approx_cost (two people) and in the present
in the data. Supervised learning analyzes the training data and provides an inferred function, which is used for
mapping new examples.
Also, this problem can be solved using Regression technique, which is used in Supervised Learning.
Regression or value estimation attempts to estimate or predict the numeric value for each individual.
Regression involves numeric target value, which is approx_cost(for two people) in our case.

1. **Decision tree** :- Tree-based learning algorithms are considered to be one of the best and mostly used
supervised learning methods. A decision tree is an algorithm which uses conditional and controls statement
that uses a tree-like graph or model for decisions and their possible consequences. We have used decision tree with max_depth=8 and random_state=0.
2. **Random Forest**:- Random forests is an ensemble classifier that consists of many decision trees and gives
the output of the class which is the most voted by individual trees. It is one of the most accurate learning
algorithms available. For many data sets, it produces a highly accurate results. It is fully parallelized and
fast to predict algorithm.We have used Random Forest with the parameter of n_estimator=100 and
max_depth=8. 
3. **Xgboost** :- It stands for eXtreme Gradient Boosting. In the bagging technique, trees are built in the parallel
while in the boosting technique trees are buile in the sequencial order and each tree tries to reduce the
error generated by the previously built tree. In Xgboost we train each subset in the form of residual
meaning difference between the true value and predicted values. XGBoost is very fast as it utilizes the
power of parallel processing uses multiple CPU to execute the model. XGBoost has in-built L1 norm and
L2 norm which prevents the model from overfitting. XGBoost has a good capability of handling missing
values.


**Different Type of Errors**
---
1.**Mean Square Error** :- Mean Square Error is the average of the square of the errors. The larger the number is
we can say that the error is large. The lower value of MSE indicates that the model is performing the better and
0 means the model is perfect Error means the difference between the observed values(y1,y2,…) and predicted
values(p(y1), p(y2),….). This error always gives a positive result because we square the difference between the
observed values and predictive values.
MSE= 1/n Σ (y – y^ )2
n = total number of data points
(y – y^ ) = difference between the actual value and predictive value

2.**R2 Score**:- R2 score varies between 0 to 100 percent. If R2 score is 100 percent, we can say that there is no
variance and variables are perfectly co-related with each other. A lower value of R2 indicates that there is a low
level of correlation. In our case, the highest value for the R2 score is for Xgboost model, and that is around
75%.

3.**Mean Absolute Error** :- In Mean Absolute Error, we calculate residual at every data point and consider the
absolute value so that positive and negative values do not cancel out each other. We can say that mean
absolute error describes the magnitude of the Residual. Mean Absolute Error is less sensitive to the outliers.
MSE= 1/n Σ |y – y^ |
n = total number of data points
|y – y^ | = absolute value of the difference between actual output and predictive output.

**Cross-Validation** 
---
It is used to evaluate the predictive performance of the models and to decide how they perform
with an unseen data set, which is also known as the test data set.The motivation behind the cross-validation is
when we build the model, it is based on the training data set. So, we do not have any idea regarding whether
our model is robust or not when it works with unknown data.
There two types of cross-validation we can perform: leave one out and k fold.We have used k fold cross
validation as leave one out requires higher computational demand.In k fold validation, we have used the value
of k as a 10 and find the value of the mean of accuracy and standard deviation for every model. The lowest
standard deviation is observed in the Xgboost, and the highest standard deviation is observed in the decision
tree. The mean of accuracy for the cross-validation is the highest for xgboost model(~73).So, we can conculde
that XGBoost provides the best performance in K fold cross validation.

**Learning Curve**
---
Learning curve is defined as a plot which is used for generalization performance against training data. We have
plotted learning curve for decision tree, random forest and XgBoost.
When the training set size is 1, we can see that the error for the training set is 0 since the model has no
problem fitting perfectly a single data point. But when tested on the test set, the error goes up to 233.5,236, and
240 for Decision Tree Regressor, Random Forest Regressor and XGBoost Regressor repectively. This
behaviour is due to the same data point which makes the prediction perfect.

**Grid Search**
---
There are two types of parameters which are commonly used in the field of machine learning and data
Science,. The first type of parameters which are learned from the model and the second type of parameters
which can be passed to the model for the tuning and also known as hyperparameters.
While building our model using random forest or Xgboost, we used different parameters such as n_estimator
and maximum depth. Generally, we randomly select the values of these parameters and check what our model
returns as a result. But, many times, selecting parameters for the algorithm randomly can make our model
exhaustive.
So rather than randomly giving the parameters to the model and checking the result every time, the best
practice is to build an algorithm which gives the parameters and its value in which our model works the best.
This algorithm is called a grid search.

Parametparameters used for tunning in decisiontree:- 

1. **max_depth**:- The maximum depth of the tree.Increasing its value will make the model overfit
2. **min_impurity_decrease** :- A node will be split only if this split produces a decrease of the impurity greater
than equal mentioned value.
Parametparameters used for tunning in random forest:- parameters={'max_depth':
range(1,20,1),'n_estimators':[100,125,150]}
1. **n_estimators** :- Number of trees in the forest.
Parametparameters used for tunning in XGBoost:- parameters={'colsample_bytree':[0.1,0.2,0.3],
'max_depth':[3,4,5],'subsample':[0.3,0.4,0.5],'n_estimator':[100,120]}
1. **colsample_bytree** :- It is a subsample ratio of the column while constructing a tree.
2. **Max_depth**:- It indicates the maximum depth of the tree. Increasing its value will make the model overfit.
3. **n_estimator**:- number of trees to be used in the forest.
4. **Subsample**:- It occurs once in every boosting iteration. It is the ratio of the training instances. If we take its
value as 0.5, then Xgboost will randomly sample the 50% of training data before growing the trees to
prevent overfitting.

After Running the GridSearch, we have received the tunned parameters for all the regressor. If we give this
tuned parameter into our model then the performance will increase.

Also, during the grid search we have calculated the best score for every regressor which is greater than K fold
accuracy of the every regressor. So, we can say that overall performance of the model has been improved.
